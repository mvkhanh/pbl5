from dataset import get_dataloader
import torch
import torch.nn as nn
from model import get_model
from mymodel import MyModel
import os
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve

# ---------------------- C·∫•u h√¨nh ----------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

BATCH_SIZE = 32

CHECKPOINT_PATH = "ckpt/best_model.pth"

train_abnormal_path = 'UniformerData/Train/Abnormal/'
train_normal_path = 'UniformerData/Train/NormalVideos/'
test_normal_path = 'UniformerData/Test/NormalVideos/'
test_abnormal_path = 'UniformerData/Test/Abnormal/'

def eval1(model, loss_fn, data_loader):
    """ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test v√† t√≠nh Precision, Recall, F1-score."""
    model.eval()
    correct = 0
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.inference_mode(), torch.autocast(device_type=DEVICE, dtype=torch.float16):  # ‚úÖ D√πng float16
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            
            outputs = model(inputs)  # Forward pass
            
            # T√≠nh loss
            loss = loss_fn(outputs, labels)
            total_loss += loss.item()
            outputs = torch.sigmoid(outputs)
            # Chuy·ªÉn output th√†nh nh√£n d·ª± ƒëo√°n (0 ho·∫∑c 1)
            preds = (outputs > 0.5).float()
            
            # L∆∞u l·∫°i d·ª± ƒëo√°n v√† nh√£n th·∫≠t
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Chuy·ªÉn v·ªÅ numpy array
    all_preds = np.array(all_preds).flatten()
    all_labels = np.array(all_labels).flatten()

    # T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°
    precision = precision_score(all_labels, all_preds)
    recall = recall_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    acc = (all_preds == all_labels).mean() 

    avg_loss = total_loss / len(data_loader)

    return avg_loss, acc, precision, recall, f1

def load_checkpoint(model, checkpoint_path):
        """N·∫°p l·∫°i tr·∫°ng th√°i t·ª´ checkpoint."""
        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)
        model.load_state_dict(checkpoint['model_state'])
        print(f"üîÑ Load successfully!")

if __name__ == '__main__':
    train_loader, test_loader = get_dataloader(train_abnormal_path, train_normal_path, batch_size=BATCH_SIZE, split_size=0.15)
    # Model
    model = MyModel().to(DEVICE)

    if os.path.exists(CHECKPOINT_PATH):
         load_checkpoint(model, CHECKPOINT_PATH)

    all_labels = []
    probs = []  # L∆∞u x√°c su·∫•t thay v√¨ nh√£n nh·ªã ph√¢n

    model.eval()
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            outputs = model(inputs)  # üîπ Logits
            prob = torch.sigmoid(outputs)  # ‚úÖ Chuy·ªÉn logits th√†nh x√°c su·∫•t

            all_labels.extend(labels.cpu().numpy())
            probs.extend(prob.cpu().numpy())

    # Chuy·ªÉn sang numpy
    all_labels = np.array(all_labels)
    probs = np.array(probs)
    

    precisions, recalls, thresholds = precision_recall_curve(all_labels, probs)
    np.save('all_labels.npy', all_labels)
    np.save('probs.npy', probs)
    # T√¨m threshold c√≥ F1-score cao nh·∫•t (Precision * Recall l·ªõn nh·∫•t)
    optimal_threshold = thresholds[np.argmax(precisions * recalls)]

    print(f"Optimal threshold: {optimal_threshold:.4f}")
