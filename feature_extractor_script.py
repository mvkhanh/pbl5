import os
import subprocess
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
import shutil
from transforms import (
    GroupCenterCrop,
    ToTorchFormatTensor,
    GroupNormalize,
    Stack,
    GroupScale
)
from uniformer import uniformer_base
from huggingface_hub import hf_hub_download

# C·∫•u h√¨nh thi·∫øt b·ªã
device = 'cuda'
model = uniformer_base()

# Load model
model_path = hf_hub_download(repo_id="Sense-X/uniformer_video", filename="uniformer_base_k600_32x4.pth")
state_dict = torch.load(model_path, map_location='cpu')
model.load_state_dict(state_dict)
model = model.to(device).eval()

# C·∫•u h√¨nh input
crop_size = 224
scale_size = 256
input_mean = [0.485, 0.456, 0.406]
input_std = [0.229, 0.224, 0.225]

# Transform
transform = T.Compose([
    GroupScale(scale_size),
    GroupCenterCrop(crop_size),
    Stack(),
    ToTorchFormatTensor(),
    GroupNormalize(input_mean, input_std) 
])

# H√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng nhi·ªÅu segment c√πng l√∫c
def extract_features(model, batch_segments):
    """
    batch_segments: Tensor c√≥ shape [B, 3, T, H, W]
    """
    with torch.no_grad():
        batch_segments = batch_segments.to(device)
        features = model.forward_features(batch_segments)
        gap = torch.nn.AdaptiveAvgPool3d((1, 1, 1))
        features = gap(features).squeeze()  # [B, C, 1, 1, 1] -> [B, C]
    return features.cpu().numpy()

# H√†m x·ª≠ l√Ω to√†n b·ªô m·ªôt video
def process_video(model, video_path, output_path, fps_sampling=5, segment_length=32, batch_size=8):
    """
    - Tr√≠ch xu·∫•t frames t·ª´ video
    - Chia th√†nh c√°c segment (32 frames)
    - X·ª≠ l√Ω nhi·ªÅu segment trong m·ªôt batch
    - L∆∞u to√†n b·ªô features c·ªßa video v√†o m·ªôt file duy nh·∫•t
    """
    temp_frame_dir = "temp_frames1"
    os.makedirs(temp_frame_dir, exist_ok=True)

    # Tr√≠ch xu·∫•t frames b·∫±ng ffmpeg
    frame_pattern = os.path.join(temp_frame_dir, "frame_%04d.png")
    ffmpeg_cmd = f'ffmpeg -i "{video_path}" -vf "fps={fps_sampling}" "{frame_pattern}" -hide_banner -loglevel error'
    subprocess.run(ffmpeg_cmd, shell=True)

    # ƒê·ªçc danh s√°ch frames
    frames = sorted(os.listdir(temp_frame_dir))
    if len(frames) < segment_length:
        print(f"B·ªè qua video {video_path} v√¨ c√≥ √≠t h∆°n {segment_length} frames.")
        return

    # Chia th√†nh c√°c segment
    num_segments = len(frames) // segment_length
    segment_list = [frames[i * segment_length:(i + 1) * segment_length] for i in range(num_segments)]

    # Lo·∫°i b·ªè 20% segment ƒë·∫ßu v√† cu·ªëi n·∫øu kh√¥ng ph·∫£i video "Normal"
    class_name = os.path.basename(os.path.dirname(video_path))
    if class_name != "NormalVideos" and num_segments > 4:
        num_remove = int(0.2 * num_segments)
        segment_list = segment_list[num_remove:-num_remove]

    # X·ª≠ l√Ω theo batch
    all_features = []
    batch_segments = []
    for i, segment in enumerate(segment_list):
        segment_paths = [os.path.join(temp_frame_dir, frame) for frame in segment]
        img_groups = [Image.open(frame) for frame in segment_paths]
        transform_groups = transform(img_groups)  # Shape: [TC, H, W]

        # Reshape th√†nh [1, 3, T, H, W]
        TC, H, W = transform_groups.shape
        transform_groups = transform_groups.reshape((1, TC // 3, 3, H, W)).permute(0, 2, 1, 3, 4)
        batch_segments.append(transform_groups)

        # N·∫øu ƒë·ªß batch_size, ƒë∆∞a v√†o model
        if len(batch_segments) == batch_size or i == len(segment_list) - 1:
            batch_tensor = torch.cat(batch_segments, dim=0)  # [B, 3, T, H, W]
            features = extract_features(model, batch_tensor)
            all_features.append(features)
            batch_segments = []  # Reset batch

    # G·ªôp to√†n b·ªô features c·ªßa video th√†nh m·ªôt m·∫£ng numpy duy nh·∫•t
    if all_features:
        all_features = np.concatenate(all_features, axis=0)
        np.save(output_path, all_features)

    # D·ªçn d·∫πp th∆∞ m·ª•c frames
    shutil.rmtree(temp_frame_dir)

# X·ª≠ l√Ω to√†n b·ªô dataset
input_root = "Train"
output_root = "Segments"
fps_sampling = 5
segment_length = 32
batch_size = 8

for class_name in os.listdir(input_root):
    class_path = os.path.join(input_root, class_name)
    if not os.path.isdir(class_path):
        continue

    output_class_path = os.path.join(output_root, class_name)
    os.makedirs(output_class_path, exist_ok=True)

    for video_file in os.listdir(class_path):
        if not video_file.endswith(".mp4"):
            continue

        video_path = os.path.join(class_path, video_file)
        output_path = os.path.join(output_class_path, f"{video_file}.npy")

        if os.path.exists(output_path):
            print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {video_file}, b·ªè qua.")
            continue

        print(f"üìπ ƒêang x·ª≠ l√Ω video: {video_file}...")
        process_video(model, video_path, output_path, fps_sampling, segment_length, batch_size)

print("‚úÖ Done!")