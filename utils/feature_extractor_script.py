import os
import numpy as np
import torch
import torchvision.transforms as T
import cv2
from PIL import Image
from uniformer import uniformer_base
from huggingface_hub import hf_hub_download
from transforms import (
    GroupCenterCrop,
    ToTorchFormatTensor,
    GroupNormalize,
    Stack,
    GroupScale
)

# C·∫•u h√¨nh
device = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 4  # S·ªë segment x·ª≠ l√Ω c√πng l√∫c
SEGMENT_DIR = "5fps_Segments/Train"  # Th∆∞ m·ª•c ch·ª©a c√°c segment video
OUTPUT_DIR = "Segment_Features_tmp"  # N∆°i l∆∞u ƒë·∫∑c tr∆∞ng

# Load model Uniformer
model = uniformer_base()
model_path = hf_hub_download(repo_id="Sense-X/uniformer_video", filename="uniformer_base_k600_32x4.pth")
state_dict = torch.load(model_path, map_location="cpu")
model.load_state_dict(state_dict)
model = model.to(device).eval()

# Transform
crop_size = 224
scale_size = 256
input_mean = [0.485, 0.456, 0.406]
input_std = [0.229, 0.224, 0.225]

transform = T.Compose([
    GroupScale(scale_size),
    GroupCenterCrop(crop_size),
    Stack(),
    ToTorchFormatTensor(),
    GroupNormalize(input_mean, input_std) 
])

def extract_features(model, batch_segments):
    """ Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ batch segment """
    with torch.no_grad():
        batch_segments = batch_segments.to(device)  # Chuy·ªÉn sang GPU
        features = model.forward_features(batch_segments)
        gap = torch.nn.AdaptiveAvgPool3d((1, 1, 1))
        features = gap(features).view(features.shape[0], -1)  # [B, C, 1, 1, 1] -> [B, C]
    return features.cpu().numpy()

def video_to_tensor(video_path):
    """ ƒê·ªçc video v√† chuy·ªÉn th√†nh tensor c√≥ shape [3, T, H, W] """
    cap = cv2.VideoCapture(video_path)
    frames = []
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(Image.fromarray(frame))
    
    cap.release()
    
    if len(frames) != 32:
        print(f"‚ö†Ô∏è {video_path}: kh√¥ng ƒë·ªß 32 frames ({len(frames)})")
        return None
    
    # √Åp d·ª•ng transform
    frames = transform(frames)  # [3, 32, 224, 224]
    return frames

def process_segments():
    """ X·ª≠ l√Ω t·ª´ng batch ngay khi ƒë·ªß s·ªë l∆∞·ª£ng """
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    batch_segments = []

    for root, _, files in os.walk(SEGMENT_DIR):
        for file in sorted(files):
            if not file.endswith(".mp4"):
                continue

            video_path = os.path.join(root, file)
            feature_path = os.path.join(OUTPUT_DIR, file.replace(".mp4", ".npy"))

            if os.path.exists(feature_path):
                print(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {file}")
                continue  # B·ªè qua n·∫øu ƒë√£ tr√≠ch xu·∫•t

            tensor = video_to_tensor(video_path)
            TC, H, W = tensor.shape
            tensor = tensor.reshape((TC // 3, 3, H, W)).permute(1, 0, 2, 3)
            if tensor is not None:
                batch_segments.append((tensor, feature_path))

            # N·∫øu ƒë·ªß batch th√¨ x·ª≠ l√Ω ngay
            if len(batch_segments) == BATCH_SIZE:
                process_batch(batch_segments)
                batch_segments.clear()  # X√≥a d·ªØ li·ªáu sau khi x·ª≠ l√Ω

    # X·ª≠ l√Ω n·ªët n·∫øu c√≤n segment l·∫ª
    if batch_segments:
        process_batch(batch_segments)

def process_batch(batch_segments):
    """ Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho m·ªôt batch v√† l∆∞u k·∫øt qu·∫£ """
    batch_tensors = torch.stack([item[0] for item in batch_segments])  # [B, 3, 32, 224, 224]
    features = extract_features(model, batch_tensors)  # [B, feature_dim]
    
    # L∆∞u ƒë·∫∑c tr∆∞ng t·ª´ng file
    for i, (_, feature_path) in enumerate(batch_segments):
        np.save(feature_path, features[i])
        print(f"üíæ L∆∞u: {feature_path}")

if __name__ == "__main__":
    process_segments()